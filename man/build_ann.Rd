% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/build_ann.R
\name{build_ann}
\alias{build_ann}
\title{Build an Artificial Neural Network (ANN) Model for reconstruction}
\usage{
build_ann(
  input_dim,
  output_dim,
  hidden_layers = list(list(units = 40, activation = "tanh"), list(units = 64, activation
    = "tanh"), list(units = 40, activation = "tanh"), list(units = 32, activation =
    "tanh")),
  learning_rate = 0.001,
  output_activation = NULL
)
}
\arguments{
\item{input_dim}{An integer specifying the number of input features.}

\item{output_dim}{An integer specifying the number of units in the output layer.}

\item{hidden_layers}{A list of named lists, where each named list specifies:
\describe{
  \item{\code{units}}{An integer specifying the number of units in the layer.}
  \item{\code{activation}}{A string specifying the activation function for the layer (e.g., \code{"tanh"}, \code{"relu"}, etc.).}
}
The default configuration includes:
\enumerate{
  \item Dense layer with 40 units and \code{"tanh"} activation.
  \item Dense layer with 64 units and \code{"tanh"} activation.
  \item Dense layer with 40 units and \code{"tanh"} activation.
  \item Dense layer with 32 units and \code{"tanh"} activation.
}}

\item{learning_rate}{A numeric value specifying the learning rate for the \code{adam} optimizer. Default is \code{0.001}.}

\item{output_activation}{A string specifying the activation function for the output layer (e.g., \code{"sigmoid"}, \code{"tanh"}, or \code{NULL} for no activation). Default is \code{NULL}.}
}
\value{
A compiled \code{keras} model ready for training.
}
\description{
Constructs and compiles a sequential ANN model using the \code{keras3} package. TThe model allows users to define the number and configuration of hidden layers, the output layer activation function, and the learning rate for the optimizer.
}
\details{
The function builds a customable sequential ANN model where users can:
\enumerate{
  \item Specify the architecture of hidden layers, including the number of units and activation functions for each layer.
  \item Define the activation function for the output layer to suit regression or classification tasks.
  \item Adjust the learning rate for the \code{adam} optimizer to fine-tune the training process.
}
The model is compiled with \code{"mean_squared_error"} loss and \code{"adam"} optimizer, and it tracks \code{"mean_absolute_error"} as a metric.
}
\note{
This function uses 'keras3' for ANN reconstruction. If 'keras3' is not installed,
please install it using:
\code{install.packages("keras3")}
Alternatively, you may use 'linear' reconstruction, which does not require 'keras3'.
}
